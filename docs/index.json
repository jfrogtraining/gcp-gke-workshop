[
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/",
	"title": "JFrog DevOps on GCP Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome In this workshop you will learn about the JFrog Platform and how to leverage Artifactory and Xray for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud on Google Cloud Platform.\nLearning Objectives  Understand the roles of Artifactory and Xray in your software delivery life cycle (SDLC). Use the JFrog CLI in your build process. Use Cloud Build, triggers for your CI/CD solution. Use Local, Remote and Virtual Repositories in Artifactory. Publish artifact Build Info. Scan your artifacts and builds for security vulnerabilities. Deploy your application in Google Kubernetes Engine (GKE) using Cloud Deploy.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/2_devops_cloud.html",
	"title": "Software Supply Chain Security",
	"tags": [],
	"description": "",
	"content": "When we talk about software supply chain security, we are talking about the process of identifying, analyzing, monitoring, and mitigating security risks, vulnerabilities, and compliance issues posed by third-party software vendors within an organization’s supply chain.\nPoll says :\n In an average application, 85-90 percent of the codebase was open source.\n  99 percent of codebases contain at least some open source code and 75 percent used at least one vulnerable open source component.\n  74 percent, of the applications with vulnerable libraries can be fixed by just updating the libraries\n Software Supply Chain Attacks : A technique in which an adversary slips malicious code or even a malicious component into a trusted piece of software or hardware.\nDependency Typosquatting : Typosquatting attacks take place when bad actors push malicious packages to a registry with the hope of tricking users into installing them.\nDependency Confusion : A Dependency Confusion attack or supply chain substitution attack occurs when a software installer script is tricked into pulling a malicious code file from a public repository instead of the intended file of the same name from an internal repository.\nFederal executive order to all open source software operator, developer to have SBOM in order to subsidize software suplly chain attacks like Solarwinds.\nSBOM : Software Bill of Materials is a list of the “ingredients” that make up a piece of software, including libraries and modules — whether they are open source or proprietary, or free or paid — as well as information about the development tools, and CI (continuous integration) environmental variables used during the build process.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/3_workshop.html",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will demonstrate DevOps in the cloud with GKE and JFrog. We will build and deploy a containerized NPM application. Using the JFrog Platform, JFrog CLI and Google Cloud Build, we will compile our code, build our NPM package, execute a docker build and push, security scan the image and publish it to a repository. We will then deploy the image using Cloud Deploy and serve the application with Google Kubernetes Engine (GKE).\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup.html",
	"title": "Workshop Setup",
	"tags": [],
	"description": "",
	"content": "Before we get started on building, publishing and deploying our NPM application, we must set up our workshop environment. In this setup section, we will:\n Set up our GCP environment. Clone our workshop GitHub repository which contains our code. Configure our JFrog Platform instance (Artifactory and Xray).  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/5_build_publish_app.html",
	"title": "Build, Publish and Deploy the Npm App",
	"tags": [],
	"description": "",
	"content": "The JFrog CLI is a powerful tool that you can use in your CI/CD process and toolchain. It can be used to build code and publish artifacts while collecting valuable build information along the way. It greatly simplifies the publishing of the build artifacts and the build info to JFrog Artifactory. It is commonly used in automation scripts and with CI/CD software tools. In the next steps, we will use the JFrog CLI with Google Cloud Build to demonstrate how to build and publish with NPM and Docker.\nIn this workshop, we use NPM and Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/6_view_results.html",
	"title": "View Results in JFrog",
	"tags": [],
	"description": "",
	"content": "We have built and published our NPM package and Docker image. Let\u0026rsquo;s view these results in JFrog Artifactory.\n  Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages.\n  Type workshop-app and search. This will show the NPM package that was published with the JFrog CLI.\n  Click on it to view the details.   Click on the workshop-app:1.0.0 under the Published Modules tab to see the artifacts and dependencies.\n  Go back to the Packages view and search for npm-app. This shows the Docker image that was published.\n  Click on the docker npm-app listing.   This will show a list of the versions. Click on the latest version that was built.   In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.   Click on Actions drop down and you can download SBOM as SPDX or CycloneDX.   Click on Descendants tab to see at which particular layer your softaware is infected.   Click on any violation to see the details and impact in the Issue Details tab.   Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this.   Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries and features like threat contextual analysis, git repo scanning, jira integration, SBOM support for the SPDX and CycloneDX standard formats and many more.\n Close the Issue Details tab. View the Docker configuration for the image in the Docker Layers tab. On the Builds tab, click on npm_build in the list.  Then click on your most recent build. In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CLI CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory and Xray.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/8_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, we used Google Cloud Build to build JFrog CLI, then used the Cloud Build and JFrog Platform to build an application, manage the artifacts, scan the artifacts for security vulnerabilities and license compliance, and publish the artifacts of your application to a staging repository. Then we used Coud Deploy to deploy your application to GKE so that end-users can access it. The JFrog Platform and Google Cloud demonstrate how you can build a DevSecOps cloud platform to delivery your software to your end-users securily. This modernizes your software delivery life cycle enabling your organization to deliver quality software continuously and maintain a Secure Software Chain by leveraging advanced cloud services and infrastructure.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/2_devops_cloud/24_jfrog_platform_overview.html",
	"title": "JFrog Platform for DevSecOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute a secure software in the cloud. It provides DevOps teams with the tools needed to create, manage, secure and deploy software with ease. These tools cover everything from continuous integration and delivery (CI/CD), binary repository management, artifact maturity, security and vulnerability protection (DevSecOps), release management, analytics and distribution.\nWith the JFrog DevOps Platform — and in particular with JFrog Artifactory, JFrog Xray and JFrog Distribution , you can easily get all the granular data you need for an SBOM, including:\n All of your software’s transitive dependencies Detailed CI environment information Security and compliance data, including the “blast radius” of vulnerabilities and non-compliant licenses across all builds in your applications Distribution data, such as how, when and where your software was deployed, which customers received it, and more  This treasure trove of information about your software is available both via the JFrog UI and via the JFrog REST API, so you can export it to third-party tools of your choice.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production. Also it captures your buildinfo with every build which can be used to develop SBOM and keep track of software supply chain.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture. Boost your software security with features like threat contextual analysis, git repo scanning, jira integration, SBOM support for the SPDX and CycloneDX standard formats and many more.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.It also produce pipeinfo that can be used for SBOM.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nThe JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow. These tools not only track your software for vulnerability or attacks, if there is a breach these tools also helps you answer these questions like is your software impacted by the breach? which compoponent is exactly affeted and the remediations.\nRecommended reads:\nAutomatically Assess and Remediate the SolarWinds Hack\nYour Log4shell Remediation Cookbook Using the JFrog Platform\nSigned Pipelines Build Trust in your Software Supply Chain\nXray new security fetaures\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/2_devops_cloud/24_gcp_ci_cd.html",
	"title": "Google Cloud for DevSecOps",
	"tags": [],
	"description": "",
	"content": "Continuous integration and delivery (CI/CD) is the process by which your software components are built from code, integrated with dependencies, tested, released, and deployed to end-users. Self managed CI/CD pipelines put infrastructure strain on platform teams, which leads to queue times for developers. Google Cloud provides fully managed services which scale up and down to meet demand, including:\n Google Cloud Build - flexible platform for developer automation, including capabilities to build and test container and non-container artifacts Google Cloud Deploy - purpose-built continuous delivery with approval gates for GCP runtimes, including GKE. Binary Authorization - attestation based policy with runtime admission control.  These capabilities work together to improve software supply chain security through build provenance, deployment approval gates, and attestation-based policies for admission control.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup.html",
	"title": "Prepare the GCP Environment",
	"tags": [],
	"description": "",
	"content": "In this section, we will setup our GCP environment for the workshop. We will:\n Set up GCP environment for our workshop. Download our workshop code to the environment. Create GKE clusters. Secret Manager to access any secrets used by cloud build. Setup Cloud Deploy pipeline. Setup Cloud Build Trigger.  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup.html",
	"title": "JFrog Platform Setup",
	"tags": [],
	"description": "",
	"content": "Next, we will setup our JFrog Platform instance and the JFrog CLI. We will:\n Setup our JFrog Platform API credentials to be used by the JFrog CLI. Create NPM and Docker repositories. Generate access token and create secret in Secret Manager to be accessed by Cloud Build. Configure Xray policies and watches.  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/5_build_publish_app/51_build_jfrog_cli_image.html",
	"title": "Build the JFrog CLI Docker Image",
	"tags": [],
	"description": "",
	"content": "First, we will use Google Cloud Build to create a npm docker image with the JFrog CLI. This will enable us to use JFrog CLI to build npm packages.\nGoogle Cloud Build uses Docker to execute builds. For each build step, Cloud Build executes a Docker container as an instance of docker run.\n   Return to your Cloud Shell terminal and change directory to gcp-gke-workshop/jfrog-cli-docker.\n  We will use Google Cloud Build to create JFrog CLI docker image. This build command uses cloudbuild.yaml. If you look inside this file, you will find it performs following steps\n   docker build to create the docker image docker tag to tag newly created image docker login to log into your Free Tier docker push to push the newly created image to Artifactory\u0026rsquo;s Docker repository.  gcloud builds submit --substitutions=_JFROG_SERVER_NAME=\u0026quot;${JFROG_SERVER_NAME}\u0026quot;,_JFROG_USER=\u0026quot;${JFROG_USER}\u0026quot;,_JFROG_API_KEY=\u0026quot;${JFROG_API_KEY}\u0026quot; --gcs-log-dir=gs://${PROJECT_ID}_cloudbuild/clouddays --config=cloudbuild.yaml .    What\u0026#39;s going on here?   .\n  This command should result in a successful build of docker image and it should be pushed to Artifactory. "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/5_build_publish_app/52_build_docker_image.html",
	"title": "Build and Deploy the NPM Application Docker Image",
	"tags": [],
	"description": "",
	"content": "Now, we can use new JFrog CLI image to build the NPM application Docker image and push it to Artifactory\u0026rsquo;s Docker repository using Cloud Build Trigger.\n  Return to your Google Cloud Shell terminal and change directory to gcp-gke-workshop/workshop-app.\n  Use Google Cloud Build, trigger and the JFrog CLI to build the NPM Docker image. The cloud build trigger uses cloudbuild.yaml. If you look inside this file, you will find it performs following steps:\n   jfrog config add configure the JFrog CLI to access our JFrog Platform instance. jfrog npm-install use JFrog CLI to perform npm install to collect and validate NPM dependencies. jfrog npm-publish publish the npm all to npm repo in artifactory. jfrog build-publish use publish the build info. docker build build docker image of the NPM application. docker push push the newly created image to Artifactory\u0026rsquo;s Docker repository. create release create release in Cloud Deploy which deploys the app from artifactory in GKE.  The JFrog CLI is used to collect build information during this process and publish it to Artifactory.\nBefore triggering the Cloud Build, remember the kubernetes cluster we created in the beginning of the lab? It must be ready by now. Let\u0026rsquo;s establish connectivity to it with the following command  gcloud container clusters get-credentials gcpworkshop --project=$PROJECT_ID --zone=$ZONE Verify that our cluster is healthy and by showing the Kubernetes nodes.  kubectl get nodes We need to set our Artifactory registry credentials in order to pull the NPM application image. We will do this my creating Kubernetes secrets. Execute the following command, substitute your server name and JFrog Platform credentials (username and API key).  kubectl create secret docker-registry regcred --docker-server=$JFROG_SERVER_NAME --docker-username=$JFROG_USER --docker-password=$JFROG_API_KEY  You have to create this secret in other 2 clusters also: testgcpworkshop and staginggcpworkshop.\n  Now edit the cloudbuild.yaml in your fork to update versionName for access token secret which we have saved from 4.24 step and commit to trigger cloud build.\n    Let\u0026#39;s review!   .\n    Docker rate limit policies! Artifactory can help!   Docker Hub has set a new limit on data transfer beginning November 1st for free accounts: 100 pulls for anonymous users and 200 pulls for authenticated/free users for every 6 hours per IP address or a unique user.\nArtifactory can protect you from this by proxying and caching images! This reduces the number of pulls from Docker Hub.\nDocker also has a 6 month retention policy for free accounts. You can avoid that as well by using Artifactory as your private registry.\n.\n  If above cloud build passes all steps then it should result in a successful build of docker image which should be pushed to Artifactory.\nAlso the cloud build should create a release in Cloud Deploy pipeline and Deploy the app to test.\nExecute the following to see your deployed pod.  gcloud container clusters get-credentials testgcpworkshop --project=$PROJECT_ID --zone=$ZONE kubectl get pods You should see you npm-app pod.\nNow let\u0026rsquo;s get the external IP so that we can view your application. Execute the following.  kubectl get services\nThis will provide the EXTERNAL-IP.\n In your browser, go to https://\u0026lt;EXTERNAL-IP\u0026gt; to view your deployed web application.\n  Click through the self-signed certificate warning. You should see the following web application.\n  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup/411_get_gcp_environment.html",
	"title": "Get a GCP Environment",
	"tags": [],
	"description": "",
	"content": " If you have your own Google account and have your own GCP environment, skip this step.\n This workshop requires a GCP environment. If this is a JFrog and Google hosted event, your instructor will provide a link and an activation code. The following steps walk through the process of obtaining a GCP environment using the provided link and activation code. Otherwise, use your own GCP environment and skip this step.\nWhen using this temporary GCP environment at a hosted event, use an Incognito Window to avoid conflicts with your existing Google Account.\n  Open the instructor provided link in your browser. This will take you to environment registration page.  Fill out the form with your information and click Submit.  Next, click on the Launch Lab button.  It will take a couple minutes for your environment to be ready.  When ready, your environment information will be provided. Take a moment to copy these values to your notepad.   Open the Sign-in link in a new browser tab. If you are already signed into Google Cloud with an exiting account, please sign out.\n  Sign in using the account credentials provided for the environment.\n   Accept the new account agreement.\n  Accept the terms of service.\n  You are now on your Google Cloud Console.  Click on the Google Cloud Shell button at the top right of the console to open Google Cloud Shell terminal in your browser.  Click the Continue button and wait a few moments for you Google Cloud Shell Machine to be ready.  Great work! Let\u0026rsquo;s move onto the next step.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup/412_download_workshop_code.html",
	"title": "Download the Workshop Code",
	"tags": [],
	"description": "",
	"content": "The workshop code is located at https://github.com/jfrogtraining/gcp-gke-workshop GitHub repository. We will fork this repo and then clone locally in order to pull the required workshop files and scripts.\n Click on the Google Cloud Shell button at the top right of the console to open Google Cloud Shell terminal in your browser.  Click the Continue button and wait a few moments for you Google Cloud Shell Machine to be ready.  Clone this repository to your local directory with the following command.  git clone https://github.com/\u0026lt;user-name\u0026gt;/gcp-gke-workshop.git "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup/413_create_gke_cluster.html",
	"title": "Create GKE Clusters",
	"tags": [],
	"description": "",
	"content": "In this section, we will create GKE Clusters in our Google Cloud Shell using the gcloud CLI.\nThe gcloud command-line interface is the primary CLI tool to create and manage Google Cloud resources. You can use this tool to perform many common platform tasks either from the command line or in scripts and other automations.\n  In your Google Cloud Shell, execute the following command to set an environment variable for the GCP environment project ID.  export PROJECT_ID=`gcloud config get-value project` echo $PROJECT_ID In your Google Cloud Shell, execute the following command to set an environment variable for the GCP region and zonethat you would like to use.  export REGION=\u0026lt;your region\u0026gt; echo $REGION export ZONE=\u0026lt;your zone\u0026gt; echo $ZONE Execute the following gcloud CLI commands to create 3 GKE clusters for test, stage and prod.  gcloud container clusters create testgcpworkshop --zone $ZONE --release-channel=rapid --machine-type=e2-standard-2 --image-type=COS --disk-type=pd-ssd --disk-size=10 --num-nodes=1 --enable-autoupgrade --enable-autorepair --async gcloud container clusters create staginggcpworkshop --zone $ZONE --release-channel=rapid --machine-type=e2-standard-2 --image-type=COS --disk-type=pd-ssd --disk-size=10 --num-nodes=1 --enable-autoupgrade --enable-autorepair --async gcloud container clusters create gcpworkshop --zone $ZONE --release-channel=rapid --machine-type=e2-standard-2 --image-type=COS --disk-type=pd-ssd --disk-size=10 --num-nodes=1 --enable-autoupgrade --enable-autorepair --async With this command we have specified the following GKE cluster properties:\n Specified the Kubernetes release channel which controls Kubernetes cluster version updates. Rapid gets the latest Kubernete release as early as possible. Specified the Kubernetes node machine type as e2-standard-2 and set it to container optimized. Set disk storage type and size (SSD). Configured the number of Kubernetes worker nodes (we only need 1 for our workshop). Enabled auto-upgrade. Enabled auto-repair in case some part of our infrastructure fails.  It will take a few minutes to create the GKE clusters. While we wait, we can move on to set up the rest of the workshop.\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup/414_cloud_deploy.html",
	"title": "Create a Cloud Deploy Pipeline",
	"tags": [],
	"description": "",
	"content": "In this section, we will create a Cloud Deploy pipeline to deploy our app in test, stage and prod environments.\n  First select Cloud Deploy from your product catalog and enable it if it\u0026rsquo;s already not enabled.\n  Execute the following command to add clouddeploy.jobRunner role to the serice account.\n  gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=serviceAccount:$(gcloud projects describe $PROJECT_ID \\ --format=\u0026quot;value(projectNumber)\u0026quot;)-compute@developer.gserviceaccount.com \\ --role=\u0026quot;roles/clouddeploy.jobRunner\u0026quot; Now execute the below command to add the Kubernetes Developer permissions.  gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=serviceAccount:$(gcloud projects describe $PROJECT_ID \\ --format=\u0026quot;value(projectNumber)\u0026quot;)-compute@developer.gserviceaccount.com \\ --role=\u0026quot;roles/container.developer\u0026quot; Navigate to IAM , locate the cloud build service account and add below two roles if it does not have it already.  Cloud Deploy Releaser Service Account User Now in your cloud shell navigate to gcp-gke-workshop/workshop-app and update your clouddeploy.yaml with the GKE clusters created in 4.1.3 for test, stage and prod.  After that execute below command. This will register your pipeline with the Google Cloud Deploy service and with targets (test,stage and prod).  gcloud deploy apply --file clouddeploy.yaml --region=$REGION --project=$PROJECT_ID Once above command execute successfully, navigate to Cloud Deploy and you should see one pipeline with name clouddays-demo  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/41_gcp_environment_setup/415_cloud_trigger.html",
	"title": "Create a Cloud Build Trigger",
	"tags": [],
	"description": "",
	"content": "In this section, we will create a Cloud Build trigger which will trigger automatically the cloud build file on any push to the github repo and deploy to test.\n  Go to https://github.com/jfrogtraining/gcp-gke-workshop and fork this repo.\n  Navigate to Cloud Buiild and select Triggers from the left pane.\n  Click on + CREATE TRIGGER and give it a name workshop.\n  Select CONNECT NEW REPOSITORY in source, which will take you to authenticate your github repo from where you want to trigger cloud build.\n  Select Github as source, authenticate and then in Select Repository select your github account and forked workshop repo, click Connect.  Next from Configuration select Cloud Build configuration file and in Location select the repository added above, add workshop-app/cloudbuild.yaml as Cloud Build configuration file location.  Now add substituition variable used in cloud build file: _JFROG_SERVER_NAME , _JFROG_USER  Click on Create.  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/421_jfrog_free.html",
	"title": "Get a Free JFrog Platform Instance",
	"tags": [],
	"description": "",
	"content": "If you do not have access to a JFrog Platform instance, use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory and Xray.\nWhen signing up for the JFrog Platform Cloud Free Tier, ensure that you select Google Cloud.\n  JFrog Platform Cloud Free Tier   "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/422_api_key.html",
	"title": "Generate an API Key",
	"tags": [],
	"description": "",
	"content": " Remember your username and API key. We will use it again with the JFrog CLI and to set up GKE to deploy your image.\n  Go to your JFrog Platform instance at https://[server name].jfrog.io. Refer to your JFrog Free Subscription Activation email if needed. Substitute your server name.  Login to your JFrog Platform instance with your credentials.  Once logged into your JFrog Platform instance, you will be presented with the landing page.  Go to your profile and select Edit Profile.  Enter your password and click Unlock to edit the profile. In the Authentication Settings section, click the gear icon to generate an API key.  Copy the API Key. Click Save. We must set these credentials as environment variables to be used in the build later. Do that now with the following commands.  export JFROG_USER=\u0026lt;username/email\u0026gt;\nexport JFROG_API_KEY=\u0026lt;api key\u0026gt;\nexport JFROG_SERVER_NAME=\u0026lt;[server_name].jfrog.io\u0026gt;\n"
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/423_create_repos.html",
	"title": "Create NPM and Docker Repos",
	"tags": [],
	"description": "",
	"content": "Next, we will set up a NPM and Docker repositories in Artifactory.\n In your JFrog Platform instance go to Administration ► Repositories ► Repositories.  Click on New Local Repository on the right.  For package type, select Docker.  Specify clouddays for the Repository Key.   Click Create Local Repository.\n  Next, we must create NPM repositories that will be used for NPM dependencies. JFrog provides an easy Quick Setup option for this. Go to your profile and select Quick Setup.\n  Select NPM.  Select Create a new repository option.  Add clouddays as repository prefix and then click create. This create default NPM repositories including a remote repository for npmjs.  Three different types of repositories can be created: local, remote and virtual. Local repositories are physical, locally-managed repositories into which you can deploy artifacts. These are repositories that are local to the JFrog Artifactory instance. A remote repository serves as a caching proxy for a repository managed at a remote URL (which may itself be another Artifactory remote repository). A virtual repository (or \u0026ldquo;repository group\u0026rdquo;) aggregates several repositories with the same package type under a common URL. A virtual repository can aggregate local and remote repositories.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/424_create_access_token_secret.html",
	"title": "Create a sceret for access token in Secret Manager",
	"tags": [],
	"description": "",
	"content": "We will now create an admin access token and use that token to store as secret in Secret Manager in GCP, which will be used by Cloud Build.\n  In your JFrog Platform instance go to Administration ► Identity and Access ► Access Tokens.\n  Click on Generate Token, enter the User name and click on Generate. Copy and save the generated token somewhere.\n  Now go to your GCP console and search for Secret Manager. IF it snot enabled then enable it. Click on Create Secret , enter the name as jfrog-access-token , enter the secret value saved in above atep and click on Generate.  Now going back on Secret manager main page, click on the Actions of the secret just create above and select Copy Resource Id and save it soemwhere as this will be used later in CloudBuild.yaml.  We also need to give Cloud Build access to this secret, for that navigate to IAM in GCP and add the Secret Manager Secret Accessor role in the Cloud Build Service Account.  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/425_configure_xray_index.html",
	"title": "Configure a Xray Index",
	"tags": [],
	"description": "",
	"content": "  First we will have to enable Xray if its not already enabled, for that go to package view Application ► Artifactory ► Packages. Click on any package and go to Xray Data and click on enable xray.\n  Let\u0026rsquo;s configure Xray to index the new Docker repository automatically. Go to Administration ► Xray ► Settings.\n   Click on Indexed Resources.\n  Click on Add a Repository on the right.\n  Move the clouddays repository into the Included Repositories.  Click Save. You have now configured Xray to index the clouddays repository.  JFrog Xray scans your artifacts, builds and release bundles for OSS components, and detects security vulnerabilities and licenses in your software components. Policies and Watches allow you to enforce your organization governance standards. Setup up your Policies and Watches to reflect standard governance behaviour specifications for your organization across your software components.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/426_create_xray_policy.html",
	"title": "Create a Xray Policy",
	"tags": [],
	"description": "",
	"content": "Next, we will create an Xray security policy that sets the types of security violations to alert on.\n Go to Administration ► Xray ► Watches \u0026amp; Policies ► Policies.   Click on Create a Policy.\n  Give the policy a name like default-security and a description.\n   Click on New Rule at the right.\n  Give the rule a name like high-security-rule.\n  For Minimal Severity, specify High.\n   Click Save for the new rule.\n  Click Create to create the new policy.\n  Policies define security and license compliance behavior specifications. Policies enable you to create a set of rules, in which each rule defines a license/security criteria, with a corresponding set of automatic actions according to your needs. Policies are enforced when applying them to Watches. A policy is contextless, which means that it only defines what to enforce and not what to enforce it on.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/4_workshop_setup/42_jfrog_setup/427_create_xray_watch.html",
	"title": "Create a Xray Watch",
	"tags": [],
	"description": "",
	"content": "Next, we will create an Xray security watch to scan our new Docker repository.\n Go to Administration ► Xray ► Watches \u0026amp; Policies ► Watches.   Click on Set up a Watch.\n  Give the watch a name like clouddays-docker-repo-watch and a description.\n   Click on Add Repositories.\n  Move the clouddays repository into the Included Repositories.\n   Click Save.\n  Scroll down to the Assigned Policies and click on Manage Policies.\n  Drag the new policy that we created earlier, _default-securtity into the Include Policies.   Click Save.\n  Click Create to create the new watch. We have now configured Xray to scan our new Docker repository for security violations.\n  Xray Watches are the focal point for viewing and managing your security and license violations in the JFrog Platform. Watches provide you with the flexibility you need to meet your specific security and violation requirements. You select the resources you would like to scan for security vulnerabilities and compliance and determine the actions to be taken once a security vulnerability is detected.\n "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "  Your JFrog Platform Instance - The JFrog Platform instance that you used in this workshop will automatically be destroyed after the workshop. There isn\u0026rsquo;t anything you need to do. If you would like keep it, you can upgrade to one of the premium plans. Do this by clicking on the Upgrade button.   GKE Cluster - To cleanup your GKE resources, go to your GKE console and delete following resources\n GKE Cluster Key Ring and Key Cloud Build history    "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/resources.html",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "https://jfrogtraining.github.io/gcp-gke-workshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]